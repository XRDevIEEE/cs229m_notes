\metadata{18}{Haoran Xu and Lewis Liu}{Nov 17th, 2021}

We venture into unsupervised learning by first studying classical (and analytically tractable) approaches to unsupervised learning. Classical unsupervised learning usually consists of specifying a latent variable model and fitting using the expectation-maximization (EM) algorithm. However, so far we do not have a comprehensive theoretical analysis for the convergence of EM algorithms because fundamentally analyzing EM algorithms involves understanding non-convex optimization. Most analysis of EM only applies to special cases (e.g., see ~\citet{xu2016global,daskalakis2016ten}) and it is not clear whether any of the results can be extended to more realistic, complex setups, without a fundamentally new technique for understanding nonconvex optimization. 
Instead, we will analyze a family of algorithms which are broadly referred to as spectral methods or tensor methods, which are a particular application of the method of moments~\citep{pearson1894} with the algorithmic technique of tensor decomposition~\citep{anandkumar2015learning}. While the spectral method appears to be not as empirically sample-efficient as EM, it has provable guarantees and arguably is more reliable than EM given the provable guarantees.

\tnote{this paragraph require updating}
After discussing the basics of classical unsupervised learning, we will move on to modern applications of deep learning such as self-training and contrastive learning. Here we note that so-called ``semi-supervised'' and domain adaptation approaches in deep learning can often be reduced to unsupervised learning problems, so in some sense, our analysis here is related to many fields in deep learning that are not always referred to as unsupervised. 

\sec{Method of Moments for mixture models}

We begin by formally describing the unsupervised learning problem. First, assume that we are studying a family of distributions $P_{\theta}$ parameterized by $\theta \in \Theta$, where $P_{\theta}$ can be described by a latent variable model. Then, given data $x^{(i)},...,x^{(n)}$ that is sampled i.i.d. from some distribution in $\{P_\theta\}_{\theta \in \Theta}$, our goal is to recover the true $\theta$. 

Perhaps the most well-studied latent variable model in machine learning is the mixture of Gaussians. We consider the following model for the mixture of $k$ $d$-dimensional Gaussians. Let 
\begin{align}
\theta = \l ( (\mu_1, \cdots, \mu_k), (p_1, \cdots, p_k)\r ),
\end{align}
where $\mu_i\in \R^d$ is the mean of the $i$-th component and $p$ is a vector of probabilities belonging to the $k$-simplex, which represents the mixture coefficient for clusters. Formally, for $\Delta(k) \defeq \{ p: \|p\|_1 = 1, p\geq 0, p\in\R^k\}$, 
\begin{align}
    p = (p_1, \cdots, p_k) \in \Delta(k).
\end{align}
We then sample $x \sim P_\theta$ in a two-step approach: 
\begin{align}
    i &\sim \text{categorical}(p), \notag\\
    x &\sim \cN(\mu_i, I).
\end{align}
Here $i$ is called the latent variable since we only observe $x$. Here we assume the covariances of the Gaussians to be identity, but they can also be parameters that are to be learned.

There are many other latent variables that could be defined via a similar generative process, such as Hidden Markov Models, Independent Component Analysis, which we will discuss later. %, and Expectation-Maximization, but here we focus on the so-called Moment Method.

\subsec{Warm-up: mixture of two Gaussians}
We first study a simple case: the mixture of two Gaussians.
In this case, $k=2$, and we assume $p_1=p_2=\frac{1}{2}$. For simplicity, we also assume $\mu_1=-\mu_2$, that is, the means of the two Gaussians are symmetric around the origin. To simplify our notation, let $\mu_1=\mu$ and $\mu_2=-\mu$. These assumptions yield the following model for $x$:
\begin{equation}
    x \sim \frac{1}{2}\mathcal{N}(\mu,I) + \frac{1}{2}\mathcal{N}(-\mu,I).
\end{equation}
To implement the moment method, we need to complete the following two tasks:
\begin{enumerate}
    \item Estimate the moment(s) of $x$ using empirical samples.
    \item Recover parameters from the moment(s) of $x$.
\end{enumerate}

The first moment of $x$ is
\begin{align}
    M_1 &\defeq \Exp [x] \\
    &= \frac{1}{2}\Exp [x|i=1]+\frac{1}{2}\Exp[x|i=2] \\
    &= \frac{1}{2}\mu + \frac{1}{2}(-\mu) \\
    &= 0.
\end{align}
Therefore, the first moment provides no information about $\mu$. We compute the second moment as
\begin{align}
    M_2 &\defeq \Exp[xx^\top] \\
    &= \frac{1}{2}\Exp[xx^\top|i=1]+\frac{1}{2}\Exp[xx^\top|i=2]
\end{align}
To compute these expectations, consider an arbitrary $Z \sim \cN(\mu, I)$. Then,
\begin{align}
    \Exp [ZZ^\top] &= \Exp[Z] \Exp[Z]^\top + \Cov(Z) \\
    &= \mu \mu^\top + I
\end{align}
Recognizing that this second moment calculation is the same for both Gaussians in our mixture, we obtain:
\begin{align}
    M_2 &= \frac{1}{2}(\mu\mu^\top+I)+\frac{1}{2}(\mu\mu^\top+I) \\
    &=\mu\mu^\top+I
\end{align}
Since the second moment provides information about $\mu$, we can complete the two tasks required for the moment method using the second moment.

If we had access to infinite data, then we can compute the exact second moment $M_2=\mu\mu^\top+I$. Then, we can recover $\mu$ by evaluating the top eigenvector and eigenvalue of $M_2$.\footnote{This approach is known as the spectral method.} The top eigenvector and eigenvalue of $M_2$ is $\bar{\mu} \defeq \frac{\mu}{\norm{\mu}_2}$ and $\norm{\mu}_2^2+1$, respectively. 

In practice, however, we do not have infinite data. In that case, we need to estimate the second moment by an empirical average.
\begin{align}
    \widehat{M}_2=\frac{1}{n}\sum_{i=1}^nx\sp{i} {x\sp{i}}^\top
\end{align}
We can then recover $\mu$ by evaluating the top egivenvector and eigenvalue of $\widehat{M}_2$. However, we need this algorithm to be robust to errors, i.e., similar estimates, $\widehat{M}_2$, of the second moment should yield similar estimates of $\mu$. Fortunately, most algorithms we might use for obtaining the top eigenvector and eigenvalue are robust, so we can limit our attention to the infinite data case. Having outlined the moment method approach to the mixture of two Gaussians problem, we study a generalization of this problem in the sequel.

\subsec{Mixture of Gaussians with more components via tensor decomposition}

The general moment method for solving latent variable models is summarized by the following steps.
\begin{enumerate}
    \item Compute $M_1=\Exp[x]$, $M_2=\Exp[xx^\top]$, $M_3=\Exp[x\otimes x\otimes x],$ $M_4 = \cdots$. Note that $x\otimes x\otimes x$ is in $\mathbb{R}^{d\times d\times d}$ and $(x\otimes x\otimes x)_{ijk}=x_i\cdot x_j\cdot x_k$. For example, $M_{3,ijk}=\Exp[x_ix_jx_k]$.
    \item Design as algorithm $A(M_1, M_2, M_3,\dots)$ that outputs $\theta$.
    \item Show that $A$ is robust to errors in our moment estimates, i.e., we apply $A$ to $(\widehat{M}_1,\widehat{M}_2,\widehat{M}_3,...)$ in reality.
\end{enumerate}
In the sequel, we instantiate this paradigm for mixtures of $k$ Gaussians ($k\geq 3$). 

For the simplicity of demonstrating the idea, we assume $p_1 = \cdots = p_k =\frac{1}{k}$, i.e. $i \stackrel{\text{unif}} \sim[k]$, and $x\sim\mathcal{N}(\mu_i,I)$. Equivalently, 
\begin{equation}
    x\sim\frac{1}{k}\sum_{i=1}^k\mathcal{N}(\mu_i,I).
\end{equation}
In this example, we only describe steps (1) and (2) in the general algorithm described above.  

We first evaluate the first and second moments. The first moment follows from
\begin{align}
    M_1 &=\Exp[x] \\
    &=\sum_{i=1}^k\frac{1}{k}\Exp[x|i] \\
    &=\frac{1}{k}\sum_{i=1}^k\mu_i,
\end{align}
and the second moment is computed as
\begin{align}
    M_2 &= \Exp[xx^\top] \\
    &=\sum_{i=1}^k\frac{1}{k}\Exp[xx^\top|i] \\
    &=\sum_{i=1}^k\frac{1}{k}(\mu_i\mu_i^\top+I) \\
    &=\frac{1}{k}\sum_{i=1}^k\mu_i\mu_i^\top + I.
\end{align}

\subsubsection{Second moments do not suffice}
Can we recover $\mu=(\mu_1,...,\mu_k)$ from $\frac{1}{k}\sum_{i=1}^k\mu_i$ and $\frac{1}{k}\sum_{i=1}^k\mu_i\mu_i^\top$? Unfortunately, in most of the cases when $k\geq 3$, the first and second moments are not sufficent to recover $\mu$. 

One reason is the so-called ``missing rotation information'' problem. Let 
\begin{equation}
    U =\begin{bmatrix} \vrule & & \vrule \\ \mu_1 & \cdots & \mu_k \\ \vrule & & \vrule \end{bmatrix} \in\mathbb{R}^{d\times k}
\end{equation}
denote the matrix we aim to recover. Then, consider some rotation matrix $R\in\mathbb{R}^{k\times k}$. We consider $U$ versus $U R$:
\begin{align}
    \frac{1}{k}\sum_{i=1}^k\mu_i\mu_i^\top &= \frac{1}{k}U U^\top \\
    &=\frac{1}{k}(U R)(U R)^\top &\text{($RR^\top=I$)}
\end{align}
This result proves that the second moment is invariant to rotations. To prove a similar claim for the first moment, we also constrain our choice of $R$ such that
\begin{align}
    R\cdot\Vec{1}=\Vec{1}
\end{align}
Then,
\begin{align}
    \frac{1}{k}\sum_{i=1}^k\mu_i&=\frac{1}{k} U \cdot\Vec{1} \\
    &=\frac{1}{k} U R\cdot\Vec{1}
\end{align}
Therefore, the first and second moments of $U$ and $U R$ are indistinguishable, and we must consider the third moment in order to identify $U$.

\subsubsection{Computing the third moment}

The third moment is the tensor $\Exp[x \otimes x \otimes x] \in \mathbb{R}^{d \times d \times d}$. To express this expectation in terms of tractable quantities, we can condition on the Gaussian observed and average:
\begin{align}
	\Exp[x \otimes x \otimes x] = \frac{1}{k} \sum_{i=1}^k \Exp[x \otimes x \otimes x \mid i]
\end{align}

Each term in the sum now corresponds to the third moment for some multivariate Gaussian. Fortunately, Lemma~\ref{lec19:lma:gaussian_third_moment} suggests a formula for estimating its value.
\begin{lemma} \label{lec19:lma:gaussian_third_moment}
Suppose $z \in \cN(v, I)$. Then, 
\begin{align}
	\Exp[z \otimes z \otimes z] = v \otimes v \otimes v +  \sum_{l=1}^d \Exp[z] \otimes e_l \otimes e_l + \sum_{l=1}^d  e_l \otimes \Exp[z] \otimes e_l + \sum_{l=1}^d  e_l \otimes e_l \otimes \Exp[z] 
\end{align}
where $e_1,\dots,e_d$ denote the canonical basis vectors.
\end{lemma}
This lemma suggests that we can compute $v \otimes v \otimes v$ from a linear combination of $\Exp[z \otimes z \otimes z]$  and $\Exp[z]$. Also note that $\Exp[z] = v$. . 

\begin{proof}
We compute the third moment element-wise. That is,
\begin{align}
	(\Exp[z \otimes z \otimes z])_{ijk} &= \Exp[z_i  z_j z_k] \\
	&= \Exp[(v_i + \xi_i)\cdot (v_j + \xi_j) \cdot (v_k + \xi_k)]  &\text{$(z = v + \xi, \xi \sim \cN(0, I))$}\\
	&= v_i v_j v_k + \underbrace{\Exp [v_i v_j \xi_k] +  \Exp [v_i \xi_j v_k] +  \Exp [\xi_i v_j v_k]}_{=0} \nonumber \\ 
	&\quad + \Exp[v_i \xi_j \xi_k] + \Exp[v_j \xi_i \xi_k] + \Exp[v_k \xi_i \xi_j] + \Exp[\xi_i \xi_j \xi_k] \label{lec19:eqn:higher_moments} 
\end{align}
To explicitly compute the last four terms in \eqref{lec19:eqn:higher_moments}, we note that:
\begin{align}
    \Exp[\xi_i \xi_k] = \begin{cases} 0 &\text{if $i \neq k$} \\ \Exp[\xi_i^2] = 1 &\text{if $i = k$} \end{cases}
\end{align} 
and that for any choice of $i, j,$ and $k$,
\begin{align}
    \Exp[\xi_i \xi_j \xi_k] = 0.
\end{align}
Therefore, 
\begin{equation}
	(\Exp[z \otimes z \otimes z])_{ijk} = v_i v_j v_k + v_i \ind{j=k} +  v_j \ind{i=k}  +  v_k \ind{i=j} 
\end{equation}

Since $(\sum_{l=1}^d v \otimes e_l \otimes e_l)_{ijk} = \sum_{l=1}^d v_i  e_{lj}  e_{lk} = v_i \bbI(j=k)$, we have proven that:
\begin{equation}
	\Exp[z \otimes z \otimes z] = v \otimes v \otimes v +  \sum_{l=1}^d v \otimes e_l \otimes e_l + \sum_{l=1}^d  e_l \otimes v \otimes e_l + \sum_{l=1}^d  e_l \otimes e_l \otimes v.
\end{equation} 
\end{proof}

We can now apply Lemma~\ref{lec19:lma:gaussian_third_moment} to compute the third moment of the mixture of $k$ Gaussians. In particular,
\begin{align}
	\Exp[x \otimes x \otimes x] & =  \frac{1}{k} \sum_{i=1}^k \Exp[x \otimes x \otimes x \mid i] \\
	&=  \frac{1}{k} \sum_{i=1}^k \l (\mu_i \otimes \mu_i \otimes \mu_i +  \sum_{l=1}^d \mu_i \otimes e_l \otimes e_l + \sum_{l=1}^d  e_l \otimes \mu_i \otimes e_l + \sum_{l=1}^d  e_l \otimes e_l \otimes \mu_i \r ) \\
	&=  \frac{1}{k} \sum_{i=1}^k \mu_i \otimes \mu_i \otimes \mu_i +  \sum_{l=1}^d \frac{1}{k} \l (\sum_{i=1}^k \mu_i \r ) \otimes e_l \otimes e_l + \sum_{l=1}^d  e_l \otimes \frac{1}{k} \l (\sum_{i=1}^k \mu_i \r ) \otimes e_l \nonumber \\
    &\quad + \sum_{l=1}^d  e_l \otimes e_l \otimes \frac{1}{k} \l (\sum_{i=1}^k \mu_i \r) \\
	& =  \frac{1}{k} \sum_{i=1}^k \mu_i \otimes \mu_i \otimes \mu_i +  \sum_{l=1}^d \Exp[x] \otimes e_l \otimes e_l + \sum_{l=1}^d  e_l \otimes \Exp[x] \otimes e_l + \sum_{l=1}^d  e_l \otimes e_l \otimes \Exp[x]\\
\end{align} 

For notational convenience, let
\begin{equation}
    a^{\otimes 3} \defeq a \otimes a \otimes a.
\end{equation} 
So far, we have shown how to compute $\frac{1}{k} \sum_{i=1}^k \mu_i^{\otimes 3}$ from $\Exp[x^{\otimes 3}]$ and $\Exp[x]$. In the sequel, we will formalize the remaining problem, recovering $\{\mu_i\}_{i = 1}^k$ from $\frac{1}{k} \sum_{i = 1}^k \mu_i^{\otimes 3}$, as the tensor decomposition problem, and discuss efficient algorithms for it.

\paragraph{Tensor decomposition}
Recovering the Gaussian means, $\{\mu_i\}_{i = 1}^k$,  from the third mixture moment, $\frac{1}{k} \sum_{i = 1}^k \mu_i^{\otimes 3}$, is a special case of the general tensor decomposition problem. That problem is set up as follows: Assume that $a_1, \cdots a_k \in \bbR^d$ are unknown. Then, given $\sum_{i=1}^k a_i^{\otimes 3}$, our goal is to reconstruct the $a_i$ vectors. 

Before we present some standard results on tensor decomposition, we first describe some basic facts about tensors. Much like matrices, tensors have an associated rank. For example, $a \otimes b \otimes c$ is a rank-1 tensor. In general, the rank of a tensor $T$ is the minimum $k$ such that $T$ can be decomposed as 
\begin{equation}
    T = \sum_{i=1}^k a_i \otimes b_i \otimes c_i.
\end{equation} 
for some $\{a_i\}_{i =1}^k, \{b_i\}_{i =1}^k, \{c_i\}_{i =1}^k$.
Another difference between tensors and matrices is that the former objects do not have the typical rotational invariance. In particular, consider applying a right rotation $R\in \R^{k\times k}$ to the matrix \begin{equation}
A =\begin{bmatrix} \vrule & & \vrule \\ a_1 & \cdots & a_k \\ \vrule & & \vrule \end{bmatrix} \in\mathbb{R}^{d\times k}
\end{equation}
and get \al{\widetilde{A} = AR = \begin{bmatrix} \vrule & & \vrule \\ \tilde{a}_1 & \cdots & \tilde{a}_k \\ \vrule & & \vrule \end{bmatrix} \in\mathbb{R}^{d\times k}}

Then, 

\al{\sum_{i=1}^k a_ia_i^\top = AA^\top = (AR)\cdot (AR)^\top = \sum_{i=1}^k \tilde{a}_i\tilde{a}_i^\top}
%In particular, 
However, there is no tensor analogue to the rotation invariance result above. 
%\begin{equation}
%    AA^\top = A RR^\top A^\top.
%\end{equation}
But tensors do maintain an interesting (and useful) permutation invariance; that is, $T = \sum_{i = 1}^k a_i^{\otimes 3}$ is invariant to permutations of the indices of $a_i$. Or in other words, let $P\in \R^{k\times k}$ be a permutation matrix suppose, and let \al{
	\widetilde{A} = AP = \begin{bmatrix} \vrule & & \vrule \\ \tilde{a}_1 & \cdots & \tilde{a}_k \\ \vrule & & \vrule \end{bmatrix} \in\mathbb{R}^{d\times k}
}
Then, 
\al{
	\sum_{i=1}^k a_i^{\otimes 3} =  \sum_{i=1}^k \tilde{a}_i^{\otimes 3}
}
The lack of rotation invariance in the sense above and the existence of permutation invariance make tensor decomposition computationally challenging as well as powerful. 

We now summarize some standard results regarding tensor decomposition for $T = \sum_{i = 1}^k a_i^{\otimes 3}$. The results for decomposing the asymmetric version  $T = \sum_{i = 1}^k a_i\otimes b_i \otimes$ are largely analogous. We will not prove these results here.
\begin{enumerate}
\setcounter{enumi}{-1}
\tnotelong{Tengyu will add references here}
\item  In the most general case, recovering the $a_i$'s from $T$ is computationally hard. Any procedure will either fail to find a unique $a_i$ or it fails to find $a_i$ \emph{efficiently}. 
\item In the orthogonal case, i.e. $a_1,\dots,a_k$ are orthogonal vectors, each $a_i$ is a global maximizer of 
\begin{equation}
    \max_{\|x\|_2 = 1} T(x,x,x) = \sum_{i,j,k} T_{ijk} x_i x_j x_k
\end{equation}
We can heuristically think of $a_i$ as eigenvectors of $T$ and there exists an algorithm to compute $a_i$ in poly-time.
\item In the independent case, i.e. $a_1,\dots,a_k$ are linearly independent. Jennrich's algorithm can be used to efficiently recover $\{a_i\}_{i = 1}^k$.
\end{enumerate} 
Results 1 and 2 above both involve the so-called ``under-complete'' case ($k \leq d$), e.g., when the number of Gaussians in the mixture is smaller than the dimension of the data. Next, we describe certain overcomplete cases for which efficient tensor decomposition is possible.
\begin{enumerate}
\setcounter{enumi}{2}
\item Suppose $a_1^{\otimes2},\dots,a_k^{\otimes2}$ are independent for $k \leq d^2$. Then, applying Result 2, we can recover $a_i$ from $\sum_{i=1}^k (a_i^{\otimes2})^{\otimes3} = \sum_{i=1}^k (a_i^{\otimes6}) \in \bbR^{d^6}$.
\item Excluding an algebraic set of measure $0$, we can use the FOOBI algorithm to recover $a_i$ from the fourth-order tensor $\sum_{i = 1}^k a_i^{\otimes 4}$ when $k \leq d^2$. A robust version of the FOOBI algorithm is described in \citet{ma2016poly}.
\item Assume $a_i$ are \emph{randomly} generated unit vectors. Then, for the third order tensor, $k$ can be large as $d^{1.5}$ \cite{ma2016poly, schrammsteurer17}. 
\end{enumerate}

In summary, the moment method is a recipe in which we first compute high order moments (i.e. tensors), assume that these estimates are noiseless, and decompose these tensors to recover the latent variables. Though we do not discuss these results here, there is an extensive literature analyzing the robustness of the moment method to error in the moment estimates. Last, we note that even though we only explicitly analyze the mixture of Gaussians model here, latent variable models amenable to analysis by the moment method include ICA, Hidden Markov Models, topic models, etc.

\sec{Graph Decomposition and Spectral Clustering}
Introduced by \citet{shi2000normalized} and \citet{ng2001spectral}, spectral clustering learns a model for the data points using a \emph{pairwise} notion of similarity. Formally, assume that we are given $n$ data points $x\sp{1}, \dots, x\sp{n}$ as well as a similarity matrix $G \in \bbR^{n\times n}$ such that 
\begin{equation}
    G_{ij} = \rho (x\sp{i}, x\sp{j})
\end{equation}
where $\rho$ is some measure of similarity that assigns larger values to more similar pairs of points. 

For example, $x\sp{i}$ could denote images for which $\rho (x\sp{i}, x\sp{j})$ measures the semantic similarity. Alternatively, $x\sp{i}$ might be users of a social network and $\rho (x\sp{i}, x\sp{j}) = 1$ if $x\sp{i}$ and $x\sp{j}$ are friends. 

Our goal is to cluster the data points by viewing $G$ as a graph. For instance, in the social network example, we can naturally view $G$ as an \emph{unweighted} graph, where $G_{ij} \in \{0, 1\}$ defines the edges. Then, the clustering problem is to partition the graph into distinct neighborhoods. As we will see repeatedly in the sequel, the eigendecomposition of $G$ is closely related to this graph paritioning problem.
\subsec{Stochastic block model} 
In the stochastic block model (SBM), $G$ is assumed to be generated randomly from two hidden communities. Formally, 
\begin{equation}
    \{ 1, \cdots n \} = S \cup \bar{S},
\end{equation}
where $S$ and $\bar{S}$ partition $[n]$. Assume $|S| = \frac{n}{2}$. We then assume the following generative model for $G$. 
If $i,j \in S$ or $i,j \in \bar{S}$, then 
\begin{align}
    G_{ij} = \begin{cases}
        1 &\text{w.p. $p$} \\
        0 &\text{w.p. $1-p$} \end{cases}.
\end{align} 
Otherwise, for $i$ and $j$ in distinct components, 
\begin{align}
    G_{ij} = \begin{cases}
        1 &\text{w.p. $q$} \\
        0 &\text{w.p. $1-q$} \end{cases}
\end{align} 
for $p < q$.

Our goal is then to recover $S$ and $\bar{S}$ from $G$; the primary tool we use towards this goal is the eigendecomposition of $G$.

In some trivial cases, it is not necessary to eigendecompose $G$ to recover the two hidden communities. Suppose, for instance, that $p = 0.5$ and $q = 0$. Then, the graph represented by $G$ will contain two connected components that correspond to $S$ and $\bar{S}$.

As a warm-up to motivate our approach, we eigendecompose $\bar{G} = \Exp[G]$. Observe that
\begin{align}
    \bar{G}_{ij} = \begin{cases}
        p &\text{if $i,j$ from the same community} \\
        q &\text{o.w.} \end{cases}.
\end{align}
It is then easy to see that $\bar{G}$ is a matrix of rank $2$:
\begin{align}
    \bar{G} = \left[
        \begin{array}{c|c}
        p \cdots p & q \cdots q \\
        \vdots & \vdots \\
        p \cdots p & q \cdots q\\
        \hline
        q \cdots q & p \cdots p \\
        \vdots & \vdots \\
        q \cdots q & p \cdots p 
        \end{array}
        \right].
\end{align}
\begin{lemma} \label{lec19:lma:sbm_eigen}
Let $\bar{G} = \Exp[G]$ for the stochastic block model. Then, letting $u_i(A)$ denote the $i$-th eigenvector of the matrix $A$,
\begin{align}
    u_1(\bar{G}) &= \vec{1} \label{lec19:eqn:top_eig_G}\\
    u_2(\bar{G}) &= [\underbrace{1, \dots, 1}_{|S|}, \underbrace{-1, \dots, -1}_{|\bar{S}|}]^\top \label{lec19:eqn:second_eig_G}
\end{align}
where $u_2(\bar{G})$ has $|S|$ entries of $1$ and $|\bar{S}|$ entries of $-1$.
\end{lemma}

\begin{proof}
We begin by directly proving \eqref{lec19:eqn:top_eig_G}.
\begin{align}	
	\bar{G} \cdot \vec{1}  &= \begin{bmatrix}
           \frac{pn}{2} + \frac{qn}{2} \\
           \vdots \\
           \frac{pn}{2} + \frac{qn}{2}
         \end{bmatrix} \\
         &= \frac{p+q}{2} \cdot n \cdot \vec{1}.
\end{align}
More generally, $\vec{1}$ is the top eigenvector for any matrix with fixed row sum or any graph with uniform degree. 

Next, we prove \eqref{lec19:eqn:second_eig_G}. Let 
\begin{align}
    G' = \left[
        \begin{array}{c|c}
        r \cdots r \\
        \vdots & \makebox{\text{\huge 0}} \\
        r \cdots r \\
        \hline
        & r \cdots r \\
        \makebox{\text{\huge 0}} & \vdots \\
        & r \cdots r
        \end{array}
        \right]
\end{align}
for $r = p - q$. To precisely define $G'$, we note that $G'$ is block diagonal with two blocks of size $|S|$ and $|\bar{S}|$, respectively. Then, 
\begin{align}	
	\bar{G} &= \vec{1} \vec{1}^\top q + G'. \label{lec19:eqn:barG}
\end{align}
Thus,
\begin{align}
 G' \cdot u &= \left[
    \begin{array}{c|c}
    r \cdots r \\
    \vdots & \makebox{\text{\huge 0}} \\
    r \cdots r \\
    \hline
    & r \cdots r \\
    \makebox{\text{\huge 0}} & \vdots \\
    & r \cdots r
    \end{array}
    \right] \cdot \begin{bmatrix}
           1 \\ \vdots \\ 1\\ -1 \\
           \vdots \\
           -1
         \end{bmatrix} = r \cdot \frac{n}{2} \cdot u. \label{lec19:eqn:gprimeu}
\end{align}
Then, because $u \perp \vec{1}$, we can combine \eqref{lec19:eqn:barG} and \eqref{lec19:eqn:gprimeu} to obtain
\begin{align}
\bar{G} \cdot u =  G' \cdot u =  r \cdot \frac{n}{2} \cdot u  = \frac{p-q}{2} \cdot n \cdot u
\end{align}
Thus, $u$ has eigenvalue $\frac{p-q}{2} \cdot n$. 
\end{proof}

\begin{remark}
    Lemma~\ref{lec19:lma:sbm_eigen} shows that
    \begin{equation}
        \bar{G} = \frac{p + q}{2} \cdot \vec{1} \vec{1}^\top + \frac{p - q}{2} \cdot u u^\top.
    \end{equation}
\end{remark}
More generally, when we have more than two clusters in the graph, $G'$ is block diagonal with more than two blocks. In this setting, the eigenvectors will still align with the blocks. We illustrate this below for a generic block diagonal matrix. Let 
\begin{align}
    A = \left[
        \begin{array}{c|c|c}
        1 \cdots 1 &&\\
        \vdots & \makebox{\text{\huge 0}} & \makebox{\text{\huge 0}} \\
        1 \cdots 1 &&\\
        \hline
        & 1 \cdots 1 \\
        \makebox{\text{\huge 0}} & \vdots & \makebox{\text{\huge 0}} \\
        & 1 \cdots 1 \\
        \hline
        & & 1 \cdots 1 \\
        \makebox{\text{\huge 0}}& \makebox{\text{\huge 0}} & \vdots \\
        & & 1 \cdots 1
        \end{array}
        \right]
\end{align}

Then, the three eigenvectors of $A$ are 
\begin{align}
    \begin{bmatrix}
        1 \\ \vdots \\ 1\\ 0 \\
        \vdots \\
        0 \\ 0 \\ \vdots \\ 0
      \end{bmatrix}, \begin{bmatrix}
        0 \\ \vdots \\ 0\\ 1 \\
        \vdots \\
        1 \\ 0 \\ \vdots \\ 0
      \end{bmatrix}, \begin{bmatrix}
        0 \\ \vdots \\ 0\\ 0 \\
        \vdots \\
        0 \\ 1 \\ \vdots \\ 1
      \end{bmatrix} \label{lec19:eqn:eigenmatrix}
\end{align}
Furthermore, the rows of the matrix formed by the three eigenvectors given by \eqref{lec19:eqn:eigenmatrix} clearly give the cluster IDs of the vertices in $G$. Note also that permutations of $A$ will result in equivalent permutations in the coordinates of each of the three eigenvectors.

Next, we relate this observation to the result in Lemma~\ref{lec19:lma:sbm_eigen}. While there are no negative values in the eigenvectors given in \eqref{lec19:eqn:eigenmatrix}, we observe that any linear combination of eigenvectors is also an eigenvector, so recovering a solution that look more like \eqref{lec19:eqn:second_eig_G} is straightforward. Indeed, taking linear combinations of the eigenvectors defined above shows that there is an alternative eigenbasis that includes the all-ones vector, $\vec{1}$. How for this choice of $A$, the all-ones vector is not the unique top eigenvector. For that to be the case, we require background noise in $\bar{G}$.

In reality, we only observe $G$. In the sequel, we will show that in terms of the spectrum, $G \approx \Exp[G]$. Formally, we will leverage earlier concentration results to prove that $\norm{G - \Exp[G]}_{\text{op}}$ is small. Concretely, then,
\begin{align}
    G &= (G - \Exp[G]) + \Exp[G] \\
    &= (G - \Exp[G]) + \frac{p + q}{2} \cdot \vec{1} \vec{1}^\top + \frac{p - q}{2} \cdot u u^\top
\end{align}
Rearranging, we obtain that:
\begin{align}
    G - \frac{p + q}{2} \cdot \vec{1} \vec{1}^\top &= (G - \Exp[G]) + \frac{p - q}{2} \cdot u u^\top
\end{align}
We then hope that $G - \Exp[G]$ is a small perturbation, so that the top eigenvector of $G - \frac{p + q}{2} \cdot \vec{1} \vec{1}^\top$ is close to $u$. Namely, it suffices to show that 
\begin{equation}
    \norm{G - \Exp[G]}_{\text{op}} \ll \l \|\frac{p - q}{2} \cdot uu^\top\r \|_{\text{op}}.
\end{equation}

\metadata{20}{Miria Feng and Christopher Wolff}{Dec 1st, 2021}

We will start by proving the following lemma.
\begin{lemma}
With high probability,
\begin{align}
    \norm{ G - \Exp[G] }_{\mathrm{op}} \leq O (\sqrt{n \log n} ) \;.
\end{align}
\end{lemma}

Note that this concentration inequality is different from the ones we have seen in the course so far because both $G$ and $\Exp[G]$ are matrices, not scalars. Our goal will be to turn the quantity on the LHS into something that we are familiar with.

\begin{proof}
The key idea is that we can use uniform convergence, after noting that
\begin{align}
    \norm{ G - \Exp[G] }_{\mathrm{op}} &= \max_{\norm{ v }_2 = 1} \left\vert v^\top (G - \Exp[G]) v \right\vert \\
    &= \max_{\norm{ v }_2 = 1} \left\vert v^\top G v - v^\top \Exp[G] v \right\vert \\
    &= \max_{\norm{ v }_2 = 1} \left\vert \sum_{i, j \in [n]} v_i v_j G_{ij} - \Exp \left[ \sum_{i, j \in [n]} v_i v_j G_{ij} \right] \right\vert \;.
\end{align}
Now, the quantity inside the $\max$ is the difference between the sum of independent random variables and their expectation, which we are familiar with. We can use brute force discretization to deal with the $\max$. First, note that for a fixed $v$ with $\norm{ v }_2 = 1$, we can use Hoeffding's inequality to find that
\begin{align}
    \Pr \left( \left\vert \sum_{i, j \in [n]} v_i v_j G_{ij} - \Exp \left[ \sum_{i, j \in [n]} v_i v_j G_{ij} \right] \right\vert \geq \epsilon \right) \leq \frac{\epsilon^2}{2} \;.
\end{align}
Then, we choose $\epsilon = O(\sqrt{n \log n})$, take a discretization of the unit ball with granularity $1/n^{O(1)}$ (which yields a cover of cardinality  $\exp(n \log n)$), and take a union bound over this discretized set to achieve the desired result.
\end{proof}

\begin{remark}
Comparing this bound to $\frac{p - q}{2} \cdot n$, we can deduce that if $p - q \gg \frac{1}{\sqrt{n}}$, then we can recover the vector $u$ approximately. Via a post-processing step that we do not discuss here, $u$ can actually be recovered exactly.
\end{remark}

\subsec{Clustering the worst-case graph}

Given a graph $G(V, E)$ where $V$ denotes the set of vertices and $E$ the set of edges, we define the {\it conductance} of a component $S$ as
\begin{align}
    \phi(S) &\defeq \frac{\vert E(S, \bar{S}) \vert}{\operatorname{Vol}(S)}
\end{align}
where $E(S, \bar{S})$ is the total number of edges between $S$ and $\bar{S}$, and $\operatorname{Vol}(S)$ is the total number of edges connecting to $S$. To be precise,
\begin{align}
    E(S, \bar{S}) &= \sum_{i \in S, j \in \bar{S}} G_{ij} \\
    \operatorname{Vol}(S) &= \sum_{i \in S, j \in [n]} G_{ij} \;.
\end{align}
Intuitively, conductance captures how separated $S$ and $\bar{S}$ are. 

Since $\operatorname{Vol}(S) \geq E(S, \bar{S})$, it follows that $\phi(S) \leq 1$. Next, observe that $\operatorname{Vol}(S) + \operatorname{Vol}(\bar{S}) = \operatorname{Vol}(V)$. Then, if $\operatorname{Vol}(S) \leq \operatorname{Vol}(V)/2$, it must be the case that $\operatorname{Vol}(S) \leq \operatorname{Vol}(\bar{S})$ and therefore $\phi(S) \geq \phi(\bar{S})$. In subsequent analysis, we assume without loss of generality that $\operatorname{Vol}(S) \leq \operatorname{Vol}(V)/2$, i.e. that $S$ is the smaller community.

Next, we define $\phi(G)$ to be the {\it sparsest cut} of $G$ if
\begin{align}
    \phi(G) &= \min_{S: \operatorname{Vol}(S) \leq \operatorname{Vol}(V)/2} \phi(S) \;.
\end{align}
We might wonder why we need to normalize by $\operatorname{Vol}(S)$ in the definition of conductance. The reason for this is that $E(S, \bar{S})$ is typically minimized when $S$ is small. Thus, without this normalization, the sparsest cut would not be very meaningful.

Our goal is to find an approximate sparsest cut $\hat{S}$ such that $\phi(\hat{S}) \approx \phi(G)$.\footnote{Finding the exact sparsest cut is a computationally hard problem.} We will use eigendecomposition to approach this problem. Let $d_i = \operatorname{Vol}(\{i\})$ be the degree of node $i$, and let $D = \text{diag}(\{d_i\})$ be the diagonal matrix that contains the degrees $d_i$ as entries. Furthermore, let 
\begin{equation}
    \bar{A} = D^{-\frac{1}{2}} G D^{\frac{1}{2}}
\end{equation}
be the normalized adjacency matrix. This is equivalent to normalizing each element $G_{ij}$ of the adjacency matrix by $\frac{1}{\sqrt{d_i d_j}}$.

In most cases, it suffices to think of $G$ as a regular graph, i.e. all of its degrees are the same. Assuming $G$ is a $\kappa$-regular graph, i.e. $d_i = \kappa$; then, this normalization simply scales $G$ by $\frac{1}{\kappa}$. Let $L = I - \bar{A}$ be the Laplacian matrix. Note that any eigenvector of $L$ is also an eigenvector of $\bar{A}$. Suppose $L$ has eigenvalues $\lambda_1 \leq \hdots \leq \lambda_n$ with corresponding eigenvectors $u_1 \hdots u_n$, then $\bar{A}$ has eigenvalues $1 - \lambda_1 \geq \hdots \geq 1 - \lambda_n$ with the same eigenvectors.

\begin{theorem}[Cheeger's inequality]
The second eigenvalue of $L$, namely $\lambda_2$, is connected to the sparsest cut $\phi(G)$ as follows:
\begin{align}
    \frac{\lambda_2}{2} \leq \phi(G) \leq \sqrt{2 \lambda_2} \;.
\end{align}
Moreover, we can find $\hat{S}$ such that $\phi(\hat{S}) \leq \sqrt{2 \lambda_2} \leq 2 \sqrt{\phi(G)}$ efficiently by rounding the second eigenvector. Suppose $u_2 = [\beta_1 \cdots \beta_n]^\top \in \bbR^n$ is the second eigenvector of $L$. Then we can choose a threshold $\tau = \beta_i$ and consider $\hat{S}_i = \{ j \in [n] : \beta_j \leq \tau \}$. At least one such $\hat{S}_i$ satisfies $\phi(\hat{S}_i) \leq 2 \sqrt{\phi(G)}$.
\end{theorem}

\begin{remark}
We might wonder why this algorithm uses the second eigenvector of $\bar{A}$ in particular, and not the first eigenvector. The top eigenvector is generally not that interesting as it only captures the ``background density'' of the graph. For instance, when $G$ is $\kappa$-regular, $\vec{1}$ is the top eigenvector of $G$ and is thus also the top eigenvector of $\bar{A} = \frac{1}{\kappa} \cdot G$. 

For more general $G$, the top eigenvector of $\bar{A}$ (respectively, the smallest eigenvector of $L$) is $u_1 = [\sqrt{d_1} \cdots \sqrt{d_n}]^\top$. This fact is easily verified:
\begin{align}
    (\bar{A} \cdot u_1)_i &= \sum_j \bar{A}_{ij} \sqrt{d}_j \\
    &= \sum_j \frac{G_{ij}}{\sqrt{d_i}\sqrt{d_j}} \sqrt{d}_j \\
    &= \frac{1}{\sqrt{d}_i} \sum_j G_{ij} \\
    &= \frac{d_i}{\sqrt{d}_i} = \sqrt{d}_i
\end{align}

\end{remark}

\begin{remark}
To understand why the eigenvectors of the Laplacian are related to the sparsest cut, we examine the quadratic form the Laplacian. In particular, we have that
\begin{align}
    v^\top L v &= v^\top I v - v^\top \bar{A} v \\
    &= \sum_{i=1}^n v_i^2 - \sum_{i, j = 1}^n v_i v_j \bar{A}_{ij} \\
    &= \sum_{i=1}^n v_i^2 - \sum_{i, j = 1}^n v_i v_j \frac{G_{ij}}{\sqrt{d_i d_j}} \\
    &= 2\sum_{i=1}^n v_i^2 - 2 \sum_{(i, j) \in E} \frac{v_i}{\sqrt{d_i}} \cdot \frac{v_j}{\sqrt{d_j}} \\
    &= \sum_{(i, j) \in E} \left( \frac{v_i}{\sqrt{d_i}} - \frac{v_j}{\sqrt{d_j}} \right)^2 \;.
\end{align}
If $G$ is $\kappa$-regular, then this becomes $v^\top L v = \frac{1}{\kappa} \sum_{(i, j) \in E} (v_i - v_j)^2$. Furthermore, suppose $v \in \{0, 1\}$ is a binary vector with support $S = \operatorname{supp}(v)$. Then, 
\begin{align}
    \frac{1}{\kappa} \sum_{(i, j) \in E} (v_i - v_j)^2 &= \frac{1}{\kappa} E(S, \bar{S}) \\
    &= \frac{1}{\kappa} E(\operatorname{supp}(v), \overline{\operatorname{supp}}(v)) \;.
\end{align}
If $|\operatorname{supp}(v)| \leq n/2$, implying $\operatorname{Vol}(S) \leq \operatorname{Vol}(V)/2$, then
\begin{align}
    \frac{v^\top L v}{\norm{v}^2_2} &= \frac{\frac{1}{\kappa} E(S, \bar{S})}{\frac{1}{\kappa} \operatorname{Vol}(S)} = \phi(S) \;.
\end{align}
The term $\frac{v^\top L v}{\norm{v}^2_2}$ is also called the {\it Rayleigh quotient}. This result nicely connects the abstract linear algebraic approach to the sparsest cut approach. Note that we only achieve an approximate sparsest cut because when we compute eigenvectors, we minimize the Rayleigh quotient \emph{without any constraints on $v$}. By contrast, the sparsest cut minimizes the Rayleigh quotient subject to the constraint that $v \in \{0,1\}^n$.
\end{remark}

\subsec{Applying spectral clustering}

How do the ideas from the previous sections connect to our previous discussion of spectral clustering? Suppose that we have some raw data $x^{(1)} \cdots x^{(n)}$ that we'd like to group into $k$ clusters. \citet{ng2001spectral} propose to define a weighted graph $G$ such that each element 
\begin{equation}
    G_{ij} = \exp \left( -\frac{\norm{x^{(i)} - x^{(j)}}^2_2}{2\sigma^2} \right)
\end{equation}
represents a distance between two data points. Then, we compute the first $k$ eigenvectors of the Laplacian $L$ and arrange them into the columns of a matrix: 
\begin{equation}
    \begin{bmatrix} \lvert &  & \lvert \\ u_1 & \cdots &  u_k \\ \lvert &  & \lvert \end{bmatrix} \in \bbR^{n \times k}.
\end{equation} 
The $i$-th row of this matrix (which we denote by $v_i$) is then a $k$-dimensional embedding of the $i$-th example. Note that this is usually a much lower-dimensional representation than the raw data. Finally, we can use $k$-means to cluster the embeddings $\{v_1,\dots,v_n\}$.

In high dimensions, the issue with \citet{ng2001spectral}'s approach is that the training data points are all far away from each other. The Euclidean distance between points becomes meaningless, and so our definition of $G$ does not make sense. 

\tnote{shorten this introduction below into a 1-2 paragraph brief introduction and pointer to the future section. Move the rest to the next section; }How do we solve this issue? \citet{haochen2021provable} propose a solution. They consider an infinite weighted graph $G(V, w)$, where $w$ are the edge weights, and $V = \cX \subseteq \bbR^n$ is the set of all possible data points. For simplicity, suppose $| \cX | = N$. We define $w(x, x')$ to be large only if $x$ and $x'$ are very close in $\ell_2$ distance. Now, the graph is more meaningful, because only data points that are small perturbations of each other have high connection weights. However, we do not have explicit access to this graph, and its eigenvectors are infinite-dimensional. 

Now, suppose we have some eigenvector $u = (u_x)_{x \in \cX} \in \bbR^N$. The dimension $N$ could be very large. Rather than explicitly represent $u_x$, we represent $u_x$ by $f_\theta(x)$ where $f_\theta$ is some parameterized model. Now, the challenge is to find $\theta$ such that $[f_\theta(x)]_{x \in \cX}$ is an eigenvector of $L$. The following optimization problem,
\begin{align}
    \min_{F \in \bbR^{N \times k}} \Norm{\bar{A} - F F^\top}^2_F \;,
\end{align}
gives the top $k$ eigenvectors of $\bar{A}$. Intuitively, we are trying to fit a low-rank matrix to $\bar{A}$, and it turns out that the best fit uses the top $k$ eigenvectors. Thus, we can approximate this objective by replacing 
\begin{equation}
    F = \begin{bmatrix} \lvert &  & \lvert \\ u_1 & \cdots &  u_k \\ \lvert &  & \lvert \end{bmatrix}
\end{equation}
by
\begin{equation}
    \begin{bmatrix} f_\theta(x^{(1)})^\top \\ \vdots \\ f_\theta(x^{(n)})^\top \end{bmatrix}.
\end{equation} 
Then, the minimization problem after this substitution becomes
\begin{align}
    \min_\theta \sum_{i, j \in [N]} \left( \bar{A}_{ij} - \langle f_\theta(x^{(i)}), f_\theta(x^{(j)})\rangle \right)^2 \;.
\end{align}
To make this tractable, we can replace the sum of $i, j \in [N]$ with empirical samples from the data distribution. It turns out that the resulting objective is now very similar to an objective used in contrastive learning.


\sec{Self-supervised Learning}

\subsec{Pretraining / self-supervised learning / foundation model basics}
\subsec{Analysis of contrastive learning}